# Stylistic Analysis: Hard Problems for Robots: Embodied Cognition

## I. Structural & Organizational Analysis

| Element | Description/Examples (Quote an Example) |
| :--- | :--- |
| **Pacing** | A classic "lecture" pace. It starts with a broad, provocative thesis, slows down for a detailed, step-by-step explanation of a core concept (the neuroscience of movement), and then applies that lesson to the original thesis with concrete examples (AI hands, dancing robots). |
| **Section Flow** | Structured like a lesson plan. He uses headings as guiding questions or statements ("How to move your body," "Why can’t AI draw hands?"). The argument builds logically, with the neuroscience section serving as the foundational evidence for all subsequent critiques of AI. |
| **Introduction Technique** | **Contrarian Hook + Pop Culture Reference.** He starts by directly challenging the current AI hype and grounds his critique in a very "online" concept. Quote: *"In fact, these models are hopeless at even basic biological intelligence. Show me the ChatGPT powered [grass-touching](https://www.urbandictionary.com/define.php?term=touch%20grass) robot!"* |
| **Conclusion Technique** | **Philosophical Reflection.** He concludes by reframing the entire problem, arguing that we undervalue biological complexity. It ends on a thought-provoking, almost poetic note about the "boring miracle" of everyday embodiment. Quote: *"It’s objectively far, far harder to pick up a pen than it is to write code. We just don’t value it..."* |

## II. Rhetorical & Voice Analysis

| Element | Description/Examples (Quote an Example) |
| :--- | :--- |
| **Tone** | **Didactic and Confident.** The tone is that of an expert educator who is enthusiastic about his subject (neuroscience) and slightly dismissive of the naivety in another field (AI hype). It's patient when explaining biology, but sharp when critiquing technology. |
| **Author Persona** | **The Neuroscience Educator.** In this piece, he acts as a guide, taking the reader on a tour of the brain. He simplifies complex systems without losing accuracy, building trust through clear, step-by-step explanations. Quote: *"Did you know you have a second, smaller brain in your brain that’s just for moving your body?"* |
| **Use of Humor** | **Geeky & In-the-know.** The humor relies on nerdy asides and internet culture references. Quote: *"We put a brain in your brain so you can think while you think!"* or referencing AI faces having *"Balenciaga cheekbones."* |
| **Rhetorical Devices** | **Extended Analogy & Deconstruction.** His primary device is deconstructing a simple action ("pick up a pen") to reveal its immense underlying complexity. He uses this revealed complexity as the foundation for his entire argument against disembodied AI. |
| **Audience Relationship** | **Professor to Student.** He guides the reader, uses visuals (brain diagrams) to aid understanding, and directly addresses them to build engagement. He assumes the reader is intelligent but lacks his specific domain knowledge. |

## III. Sentence-Level & Lexical Analysis

| Element | Description/Examples (Quote an Example) |
| :--- | :--- |
| **Sentence Length** | Varied for effect. Long, detailed sentences describe the biological pathways, while short, declarative sentences deliver the analytical punchlines. Quote: *"And this is basically why AI can’t draw hands."* |
| **Paragraph Length** | Function-driven. Long paragraphs are used for the dense neuroanatomy lesson. Shorter paragraphs are used to introduce new ideas, transition, or emphasize a key takeaway. |
| **Vocabulary** | **Precise High/Low Diction.** He uses highly specific scientific terms (*proprioceptive*, *basal ganglia*, *cerebellum*) but immediately explains them in simple, accessible language. This blend makes the content feel both authoritative and easy to follow. |
| **Formatting** | Highly structured for learning. Uses **bolding** for series introductions, *italics* for key terms, and numerous images with captions that act as visual aids for the "lecture." The formatting is crucial to the article's educational goal. |
| **Citation/Grounding** | Consistent use of embedded hyperlinks to Wikipedia, scientific papers, and cultural references. This maintains a clean reading experience while providing a trail of evidence for curious readers. |

## IV. Key Takeaways on Style

*   **Signature Move:** The **"Deconstructive Explanation."** The author's core technique is taking a mundane, physical action that readers take for granted and meticulously breaking it down into its astonishingly complex neurobiological components. He then masterfully wields this newfound appreciation for biological complexity as a tool to critique the shortcomings of artificial systems.

*   **Best Example:** This passage perfectly showcases his style. He introduces a complex part of the brain (cerebellum), gives it a memorable and slightly funny nickname ("little wrinkly guy"), explains its function with a clear analogy (coordinate substitution), and directly links it back to the central theme of embodied cognition.
    > "Did you know you have a second, smaller brain in your brain that’s just for moving your body? It’s the [cerebellum](https://en.wikipedia.org/wiki/Cerebellum), which you might know as the little wrinkly guy hanging off the back of the brain... The cerebellum does a number of things, but arguably it’s most important role is translating motor plans into actual algorithms for moving muscles."

***

# Hard Problems for Robots: Embodied Cognition

This **AI critique** is the first in a series that discusses unresolved hard problems for artificial intelligence. This chapter focuses on _embodiment_ :__ appropriately using a physical body to perform tasks.

Much recent AI-related anxiety is a consequence of AI getting better at generating media. Because [ChatGPT](https://chat.openai.com/auth/login) and [Midjourney](https://www.midjourney.com/home) excel at [simulating the internet discourse](https://www.youtube.com/watch?v=iE39q-IKOzA) on which they were trained it can seem to a severely online human as if these models are generally intelligent. In fact, these models are hopeless at even basic biological intelligence. Show me the ChatGPT powered [grass-touching](https://www.urbandictionary.com/define.php?term=touch%20grass) robot!

Thanks for reading How to Get Smarter With Neuroscience! Subscribe for free to receive new posts and support my work.

Subscribe

## How to move your body.

Suppose you want to reach out and pick up a pen on your desk.

This is a fairly complex behavior. You’ll need to understand what a pen is, what a desk is, and which parts of your visual stimuli correspond to those different objects. You’ll need to distinguish the pen parts of your environment from the not-pen parts and measure the distance between your hand and the target. You’ll need to be sure to inhibit closely related behaviors, like picking up a cup, or writing with a pen. These processes are complex enough to basically involve the entire brain. However, we’re focused on the [final common pathway](https://en.wikipedia.org/wiki/Charles_Scott_Sherrington) so we can start in the [prefrontal cortex ](https://en.wikipedia.org/wiki/Prefrontal_cortex)(PfC):

[![File:Prefrontal cortex \(left\) - medial view.png - Wikipedia](https://substackcdn.com/image/fetch/$s_!U8rg!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffbccf2b8-86b1-460c-9e14-6a4095a438ca_1024x1024.png)](https://substackcdn.com/image/fetch/$s_!U8rg!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffbccf2b8-86b1-460c-9e14-6a4095a438ca_1024x1024.png)

The PfC is where this stew of competing concepts is being represented in an abstract way. Next we’ll need a system that can focus in on the specific target of grabbing a pen. These structures are mostly in the [basal ganglia](https://en.m.wikipedia.org/wiki/Basal_ganglia) embedded deep in the brain:

[![](https://substackcdn.com/image/fetch/$s_!A8Jp!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc42d57d-0d37-45df-97a7-a877bb9824bc_890x1280.png)](https://substackcdn.com/image/fetch/$s_!A8Jp!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc42d57d-0d37-45df-97a7-a877bb9824bc_890x1280.png)

These structures contain fibers that turn on or off other brain regions. Using a pretty clever system of [go/no-go logic gates](https://en.m.wikipedia.org/wiki/Cortico-basal_ganglia-thalamo-cortical_loop) they can shut down or activate the different pen/not pen representations in the PfC. Once the right configuration is amplified and the wrong ones are inhibited, the pen-reaching behavior can be passed off to the motor regions of the brain:

[![File:Motor Cortex Image.jpg - Wikimedia Commons](https://substackcdn.com/image/fetch/$s_!F2SO!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca0e6c9e-efb4-43e3-8028-c7564f6e4a02_2560x1895.jpeg)](https://substackcdn.com/image/fetch/$s_!F2SO!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca0e6c9e-efb4-43e3-8028-c7564f6e4a02_2560x1895.jpeg)

The premotor, supplementary motor, and primary motor cortices work out the specific details of what motor program the body should run to actually carry out the reaching.

But we’re not actually done. In fact, we’re still about six or seven synapses from the hand! We know we’re reaching for a pen, and we even know we’re going to use our hand to do it, but we still need _an entire additional brain_ to carry out the computations to make that reach.

Did you know you have a second, smaller brain in your brain that’s just for moving your body? It’s the [cerebellum](https://en.wikipedia.org/wiki/Cerebellum), which you might know as the little wrinkly guy hanging off the back of the brain:

[![File:Anterior lobe of cerebellum -- 04.png - Wikimedia Commons](https://substackcdn.com/image/fetch/$s_!6bM3!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a390964-208e-4541-b195-916f097fe19c_1200x1200.png)](https://substackcdn.com/image/fetch/$s_!6bM3!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a390964-208e-4541-b195-916f097fe19c_1200x1200.png)We put a brain in your brain so you can think while you think!

The cerebellum does a number of things, but arguably it’s most important role is translating motor plans into actual algorithms for moving muscles. Think of this processing like doing a coordinate substitution. Your eye looks down and sees the pen at a certain angle relative to your head, but the direction your hand has to move in is at a different angle. So all the math your motor cortex did to figure out how to reach for the pen is wrong unless you have a hand growing out of your eyeball. The cerebellum can do the translation to shift the eye’s frame of reference to the hand’s frame of reference. It will also do all the scut work necessary to change all of the visual angles to _joint angles._ Your arm is not a tentacle. You don’t just have to reach in the right direction, you have to use your muscles to set each joint to the proper position. Roboticist call these muscle movements “forward transforms,” and figuring out the right ones for mechanical arms is a real bear.

We’re still not done! The cerebellum can get us more or less to the point where our hand is at the right place and our fingers are closing in on the pen, but the precise amount of force applied, and the tiny shifts in tension on finger muscles that allow us to hold on if the pen starts to slip have nothing to do with the brain at all. Those are mostly [spinal reflexes](https://en.wikipedia.org/wiki/Stretch_reflex). The nerves in the hands and spine automatically make micro adjustments in response to sensory feedback from the touch receptors in the hand. This is not to say the brain doesn’t know about those adjustments—it does! It gets tons [mechanoreceptive](https://en.wikipedia.org/wiki/Mechanoreceptor) and [proprioceptive](https://en.wikipedia.org/wiki/Proprioception) feedback and it constantly adjusts its plans in response to this data. But the loop between brain and hand takes a quarter of a second. If the hand waited for the brain to respond to a slippery pen, we’d drop stuff.

And this is basically why AI can’t draw hands.

### Why can’t AI draw hands?

Everyone knows that AI can’t draw hands. But why is it so bad? There must be tons of pictures of hands.

[![Imagine a hand that defies the norms of human anatomy, with six fingers including two thumbs, one on each end. This bizarre hand has a swirling pattern of skin, with colors transitioning from a natural skin tone at the wrist to a vibrant blue and green at the fingertips. The nails are elongated and slightly curved, resembling claws, and each finger is adorned with small, glowing gems embedded in the skin. The hand is posed in a dynamic gesture, as if reaching out to grasp something unseen, showcasing the unusual flexibility and range of motion of its extra digits.](https://substackcdn.com/image/fetch/$s_!mKGn!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fabf32481-07b7-4516-9989-698535bdd912_1024x1024.webp)](https://substackcdn.com/image/fetch/$s_!mKGn!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fabf32481-07b7-4516-9989-698535bdd912_1024x1024.webp)

[This](https://www.youtube.com/watch?v=JAia6Qi-ByA) video indulges in the most common, wrong explanation: human brains are especially attentive and sophisticated in how they process hands. This is garbage. It’s cargo-cult neuroscience at best. The brain does devote a ton of real-estate to hands, but in _[sensory-motor](https://en.wikipedia.org/wiki/Cortical_homunculus)_ regions not visual ones. Besides, human brains devote even more space to _faces_ and in both sensory-motor and visual processing areas. Yet AI faces are comparatively pretty good—the best way to detect them is to check for [Balenciaga](https://www.youtube.com/watch?v=iE39q-IKOzA) cheekbones. In any case this explanation gets us nowhere. Saying that humans dislike AI hands because they are sensitive to depictions of hands is begging the question.

Besides, the explanation of hands fails to answer another critical AI art question: why does Batman have three legs?

[![Create an image in the classic 1950's black and white Silver Age comic book style, featuring a superhero with a unique twist. This character wears a costume with a bat logo on the chest and a cowl with pointed ears, reminiscent of the iconic superhero aesthetics from that era. However, diverging from the norm, this superhero is designed with at least three legs, each positioned to suggest rapid motion, as if the character is running. The costume, detailed with the vintage flair of the 1950s comics, includes a utility belt and a cape flowing behind. The background should be minimal, focusing on the character's dynamic, action-packed posture and the innovative anatomy.](https://substackcdn.com/image/fetch/$s_!CA7R!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2774dd5f-5e9a-4b94-8c30-6b799fe62547_1024x1024.webp)](https://substackcdn.com/image/fetch/$s_!CA7R!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2774dd5f-5e9a-4b94-8c30-6b799fe62547_1024x1024.webp)

AI struggles with depictions of bodies because bodies are highly _configurable._ It’s valid for Batman’s leg to be in either position depending on where he is in his stride. The model needs to make a tricky judgement call about how to render the image. This is a technique that cartoonists play with as well: it’s valid to draw Batman with what appears to be three legs so long as you add some motion blur to suggest movement.

Hands are even more configurable than feet. There are many valid positions for fingers, and they move a lot. Both stable diffusion models like Midjourney and Transformer based models like DALL-E draw by inferring continuations of edges and forms. First, the model draws the average of many hands—a blob—and then refines that blob by adjusting with data from more specific contexts. In other words, they draw a hand a lot like this:

[![Hand Tutorial by AJ-illustrated on DeviantArt](https://substackcdn.com/image/fetch/$s_!F0ZP!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f694385-9bc4-44c4-89b5-648606466964_1024x1457.jpeg)](https://substackcdn.com/image/fetch/$s_!F0ZP!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f694385-9bc4-44c4-89b5-648606466964_1024x1457.jpeg)Full guide [here](https://www.deviantart.com/aj-illustrated/art/Hand-Tutorial-733805218).

Another thing to notice is how ordinary language usually does not describe physical body positions. If I prompt DALL-E by saying “draw an artist” I haven’t specified if they are holding a pencil, and I certainly haven’t specified if they’re using a [dynamic tripod or palmer supinate grip](https://teachhandwriting.co.uk/grip-development.html) to hold that pencil. If you do go out of your way to use a specific prompt like “Draw a left hand, as viewed from above. it should have exactly five fingers, the joints of each finger should be held straight.” You get a pretty good rendering. Except that it’s a right hand.

[![A realistic left hand viewed from above, with five fingers straight and joints visible. The skin, nails, and anatomical details are focused on to highlight the natural structure and positioning of a human hand in a neutral stance.](https://substackcdn.com/image/fetch/$s_!bN32!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7db3a5de-6ef9-4b68-9555-04d8e3e1bdaf_1024x1024.webp)](https://substackcdn.com/image/fetch/$s_!bN32!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7db3a5de-6ef9-4b68-9555-04d8e3e1bdaf_1024x1024.webp)

It’s really difficult to describe specific configurations of physical bodies with language! In fact, it would be awfully nice if I had a real physical hand attached to me that I could use as a reference. Even better if I had a huge system of neural processing with tons of specialized processing units dedicated to how to configure that hand.

### How to compute with your body.

Much of human (and animal, and robot) cognition is _embodied_. That is, the reasoning and representations used by biological brains refer to the specific sensory-motor assemblies of our physical bodies. Because we have a body we usually don’t need to learn explicit representations for how bodies work—we can use our own body as a simulation.

If I want to imagine how a hand looks when holding a pen, I can just pick up a pen and use my own body as a reference. I’m so familiar with my body that I usually don’t even need to go that far, the configuration is immediately intuitive. I’m so plugged into the physics of my own body, that I can run physical simulations using that system effortlessly and save all of the computation that I’d normally need to use my brain for. A computer scientists might call my body a “specialized quantum computer purpose-built for physics simulations,” and that would be an accurate, albeit awkward way of describing the state of affairs.

Some embodied processing is pretty obvious: controlling a robot body accurately obviously requires a pretty elaborate representation of that body. But some is more subtle—our bodies are loaded with sensory-motor feedback loops that allow us to do [reposition our grip](https://www.ncbi.nlm.nih.gov/books/NBK553133/) without thinking and perceive subtle [textural differences](https://www.pnas.org/doi/10.1073/pnas.1818501116). How broadly this embodied processing contributes to human cognition is a subject of debate. There are folks who argue that nearly every aspect of human thought depends on some kind of embodied processing—One example that particularly sticks with me is the claim that we understand fluid dynamics [because we take baths](https://www.sciencedirect.com/science/article/abs/pii/0010027796007111)—but even if you don’t go as far as metaphor it’s inarguable that having a body is a really handy for many kinds of real-world predictions.

All of this creates some fairly substantial challenges for AI. It’s one thing to draw a hand by configuring pixels on a screen, it’s much harder to draw a hand _using a hand_. If AI is going to have meaningful impact in the physical world it will need learn to control real physical bodies. And there are some strong reasons to believe that the way that biological bodies work is about as good as it gets. Superior robot embodied cognition is _never_ going to happen.

“Never” is a pretty strong claim that deserves some unpacking. After all, what about [this](https://www.bloomberg.com/news/articles/2020-08-13/goodbye-to-bartenders-robots-could-soon-make-your-drink?leadSource=reddit_wall) robot bartender? What about these dancing robots?

Don’t get me wrong, these applications are super awesome, and definitely count as AI controlling bodies under challenging circumstances. The single-foot balance at 1:38 probably made some robotics engineering students fall out of their chairs. This is Boston Dynamics showing off, and they deserve it. But there’s some sleight of hand here. These robots are running a specific list of macros in controlled conditions. They have millimeter precision for this dance routine and this invites the audience to imagine that in highly similar circumstances—with a different dance, with a slightly slippery floor—they would also perform well. When in fact, this performance is extremely brittle and would fail catastrophically if there was a light breeze.

#### Non-uniform bodies

Think back to the complicated transforms the cerebellum had to estimate to translate reaching for a pen to actual joint angles. Now imagine that the pen was always exactly 10 cm directly to the right of the center of your hand. That would be way easier! You could always run exactly the same reaching program. Sure it would be a complicated list of numbers to translate between eyes and hands, but the numbers would always be the same. There wouldn’t be any calculation, just lookup.

That’s exactly what Boston dynamics is doing with it’s dancing robots. Those movements are big look-up tables that say things like “set the knee joint to 45 degrees for 3 seconds.” Current robotic systems mostly work by running lists of specific movement macros that are tightly adapted to the specific task. These macros often don’t even include feedback sensors other than a measurement of how hard the servomotors are working. Occasionally coupled with accelerometery from gyroscopes. This is beginning to change! Roboticists are rapidly working to develop new [sensors](https://www.mdpi.com/1424-8220/22/12/4653), materials, and tools that produce a better sense of the position of their machines in space. It’s also not entirely clear that this needs to be solved in exactly the same way as biological systems do. While we use lots of sensors embedded in our joints and muscle tissue many robots use a cloud of infrared light to estimate the position of their limbs. That’s a clever alternative.

But these sensory-motor advances still have a major blind spot: they assumes that the _body_ doesn’t change.

The brain not only has to make calculate new transforms to cover a huge diversity of circumstances, but it also has to re-estimate whenever the body changes. If you attempt to apply a model fit for one body to a different one the model will make very predictable errors. You probably experienced this yourself if you ever went through a major growth spurt. Suddenly you were very clumsy as your body tried to reach out for things with arms slightly longer than your brain was used too. And everyone experiences this at birth because the brain has no idea what kind of body it’s going to end up in before that body has actually grown itself. That’s a big part of the reason the brain has so many partially redundant systems and sensor-motor feedback loops. When you’re working with a self-assembling system produced by evolution it’s got to work. It’s got to work, buddy or you’re dead. It’s ok if it’s a little clumsy, but it can’t fail catastrophically like this:

As [Rodney Books](https://en.wikipedia.org/wiki/Rodney_Brooks) at MIT famously pointed out embodied sensory-motor loops are hard to build, and they’re evolutionary older than anything cognitive. It’s not an insurmountable challenge, but it took evolution billions of years to make body sensors, and only a few hundred million to get from there to brains capable of calculus. The fact that AI has stumbled into competence at calculus and language is not evidence of sophistication in walking. We should be realistic about the difficulty of the embodied engineering challenge.

I think the engineering for embodied cognition is currently happening. There are technologies currently being developed to create sophisticated physical robots. The same kind of statistical, generative models that work for text and images can also work for embodied systems and have been applied in some limited circumstances. There are a couple of serious, but surmountable stumbling blocks. First, the amount of motor data that has been collected and digitized—such as joint position, muscle load, and other proprioceptive information—is trivial compared to the amount of digitized text and images. Second, the motors and sensors that generate and collect such data are incredibly primitive relative to keyboards and cameras. Lidar scanners and embedded kinesthetic sensors will probably have to be common place before we get really robust dancing robots.

But even when we do create the requisite sensors, collect the necessary data, and train the right models, we’ll still be in a position very much like biological systems. No matter how good your model is, it only works for the body it learned to control. A different body requires a different model, or at least some fine-tuning to adapt what was previously learned to the new instantiation. Tiny differences in friction in joints lead to extremely clumsy fingers. After a few months of wear, robots will have to re-learn. Human manual dexterity is an absolute marvel in the animal world and it’s hard to learn, it takes 15-20 years before kids aren’t completely clumsy. And that skill only lasts about 50 years before the joints start to accumulate damage and freeze up.

#### Real brains for a real world

All of this still only just scratches the surface of what embodied cognition really means. For humans and animals the most important sensory-motor loops are actually _chemical._ Olfaction and gustation are the obvious ones, but more important are probably the internally-facing sensors that measure blood glucose, blood pressure, hydration, electrolytes, circulating lipids, temperature, and so on.

Most critical of these are probably [nociceptors](https://www.ncbi.nlm.nih.gov/books/NBK10965/)—pain detectors. I had a colleague at UIUC who got a multi-million dollar humanoid robot that he hoped to teach to speak using deep neural nets and natural language processing models. It was a very cute little guy named “Bert.”

Bert spent the first three years of his life trying not to rip off it’s own very expensive arms. The millions of dollars did not go toward an embodied pain sense. And take it from people who don’t experience pain: [life without pain is hell.](https://www.cnn.com/2006/HEALTH/conditions/01/27/rare.conditions/index.html?section=cnn_latest)

Ironically, humans extreme body competence makes them somewhat poor judges of AI capabilities. Drawing with a pen is tremendously more difficult than simply coloring pixels (which is why indie game devs love [pixel art](https://en.wikipedia.org/wiki/Pixel_art)). Handwriting is more difficult than typing (no one reading this blog could decipher my handwriting). Because generative AI produces high-quality images and text, we tend to assign it some of our own body competence. It’s a strange sort of reverse-Turing test, where human’s ingratitude toward the miracle of their own body leads them to dramatically overestimate the sophistication of an AI without a body.

Moving bodies is a hard problem. There’s a ton of awesome, heavy-duty engineering going on in this space. It’s a great area to get into now because this is likely to be a decades- or centuries-long effort. And at the end of that work we will still have imperfect robot bodies that need constant repairs and fine-tuning, just like real bodies do. Because they exist in the real world and the real world full of rough parts, slippery parts, and sharp edges.

It’s objectively far, far harder to pick up a pen than it is to write code. We just don’t value it because it’s the kind of boring miracle that any healthy kid can do.

Thanks for reading How to Get Smarter With Neuroscience! Subscribe for free to receive new posts and support my work.

Subscribe
